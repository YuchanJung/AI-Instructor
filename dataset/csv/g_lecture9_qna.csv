Question,Answer
What is the primary goal of GANs?,To generate realistic samples by learning the data distribution.
How do GANs operate?,They involve a game between a generator and a discriminator.
Why are GANs considered implicit models?,They generate samples without modeling probability distributions.
What role does the generator play in GANs?,It generates fake data to fool the discriminator.
How does the discriminator function in GANs?,It distinguishes between real and generated data.
What kind of noise is used to generate data in GANs?,"Latent noise, typically from a Gaussian distribution."
Why do GANs need latent noise?,To model the entropy and variability of data.
What does the discriminator try to maximize?,The probability of correctly classifying real and fake data.
Why do we alternate between training the discriminator and generator?,To improve both the classification and generation tasks.
What is mode collapse in GANs?,When the generator focuses on limited data diversity.
Why is training GANs challenging?,Because the generator and discriminator need to balance each other.
How does the generator fool the discriminator?,By generating data that looks real to the discriminator.
Why do we perform multiple steps of gradient descent on the discriminator?,To maximize its ability to classify real versus fake.
What is BigGAN known for?,Generating photorealistic high-resolution images.
What is StyleGAN?,A GAN model that generates high-quality faces.
How are images generated by GANs evaluated?,Using metrics like Inception score and FID.
What is Wasserstein GAN?,A variant of GANs optimizing the Wasserstein distance.
Why is the Jensen-Shannon divergence important in GANs?,It's minimized when the distributions are the same.
How does the Inception score evaluate GANs?,By comparing generated images to a pre-trained classifier.
What are conditional GANs?,GANs that generate data based on specific input labels.
Why are FID scores used in GAN evaluation?,They measure similarity between generated and real data features.
How does self-attention help in image generation?,It improves global coherence in generated images.
What is the truncation trick in BigGAN?,It adjusts noise to trade between variety and quality.
How do we prevent mode collapse?,By ensuring the generator captures diverse data patterns.
What is the key advantage of Wasserstein distance?,It provides smoother gradients for training.
Why do GANs struggle with human faces?,Faces are highly sensitive to small errors.
What is the main challenge in training GANs?,The instability between the generator and discriminator.
How do GANs generalize beyond the training data?,"By learning underlying data distributions, not memorizing."
What is the Frechet Inception Distance (FID)?,It measures differences between real and generated data.
How are loss functions different in GANs?,They are learned through the game between G and D.
